线性代数 矩阵特征值 与 图像恢复

在线性代数的最后，我们都会学矩阵的特征值分解，
我们知道一个方阵A经过特征值分解后就得到
特征向量和特征值了。
那么，这个所谓的特征值和特征向量到底是什么东西呢？



我们一上来就会学到这样的一个公式：

Ax = λx，其中x是一个向量

这个式子是如此的简单粗暴，以致于从这个公式来看，给向量x乘上一个矩阵A，
只是相当于给这个向量乘上了一个系数λ。
偌大一个矩阵A对向量x的作用竟然本质上不过只是和一个小小的数字λ相同而已！！！



所以这个矩阵分解方法到底具有什么样的意义？



首先给出概念上的一种解释。所谓的特征值和特征向量，
最重要的是理解“特征”这两个字，特征向量翻译为eigen vector, eigen这个单词来自德语，
本义是在“本身固有的，本质的”。
纯数学的定义下，并不能很明白地理解到底为什么叫做特征值和特征向量。
但是举一个应用例子，可能就容易理解多了。



在图像处理中，有一种方法就是特征值分解。
我们都知道图像其实就是一个像素值组成的矩阵，假设有一个100x100的图像，
对这个图像矩阵做特征值分解，其实是在提取这个图像中的特征，
这些提取出来的特征是一个个的向量，即对应着特征向量。
而这些特征在图像中到底有多重要，这个重要性则通过特征值来表示。
比如这个100x100的图像矩阵A分解之后，
会得到一个100x100的特征向量组成的矩阵Q，
以及一个100x100的只有对角线上的元素不为0的矩阵E，
这个矩阵E对角线上的元素就是特征值，而且还是按照从大到小排列的（取模，对于单个数来说，其实就是取绝对值），也就是说这个图像A提取出来了100个特征，这100个特征的重要性由100个数字来表示，这100个数字存放在对角矩阵E中。
在实际中我们发现，提取出来的这100个特征从他们的特征值大小来看，大部分只有前20（这个20不一定，有的是10，有的是30或者更多）个特征对应的特征值很大，后面的就都是接近0了，
也就是说后面的那些特征对图像的贡献几乎可以忽略不计。
我们知道，图像矩阵A特征值分解后可以得到矩阵Q和矩阵E：




那么反推出去，把右边的三个矩阵相乘肯定也能得到矩阵A。既然已经知道了矩阵E中只有前20个特征值比较重要，那么我们不妨试试把E中除了前20个后面的都置为0，即只取图像的前20个主要特征来恢复图像，剩下的全部舍弃，看看此时会发生什么：

我们可以看到，在只取前20个特征值和特征向量对图像进行恢复的时候，基本上已经可以看到图像的大体轮廓了，而取到前50的时候，几乎已经和原图像无异了。
明白了吧，这就是所谓的矩阵的特征向量和特征值的作用。

（注意：特征值分解要求必须是nxn的方阵，如果不是行列相等的方阵，请使用奇异值分解）
